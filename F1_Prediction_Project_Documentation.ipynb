{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Race Position Prediction with Deep Learning\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook documents a complete deep learning project for predicting Formula 1 race finishing positions based on practice and qualifying performance.\n",
    "\n",
    "**Model Type:** Deep Feedforward Neural Network (Regression)\n",
    "\n",
    "**Performance:**\n",
    "- Mean Absolute Error: 2.53 positions\n",
    "- Within ±1 position: 34.30%\n",
    "- Within ±2 positions: 57.97%\n",
    "- Within ±3 positions: 75.85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "We start by importing the necessary libraries:\n",
    "- **pandas & numpy:** For data manipulation and numerical operations.\n",
    "- **matplotlib & seaborn:** For creating data visualizations.\n",
    "- **torch (PyTorch):** The deep learning framework used to build and run the neural network.\n",
    "- **sklearn:** Utilities for splitting data and scaling features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Training Dataset\n",
    "\n",
    "The dataset `training_dataset.csv` was created by aggregating raw F1 session data. Each row represents a single driver's participation in a specific race, containing features derived from Practice and Qualifying sessions, along with the target variable: `finishing_position`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load training dataset\n",
    "df = pd.read_csv('training_dataset.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nNumber of races: {df['meeting_key'].nunique()}\")\n",
    "print(f\"Number of drivers: {df['driver_name'].nunique()}\")\n",
    "print(f\"Total examples: {len(df)}\")\n",
    "\n",
    "# Show first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the features available for the model. We exclude metadata columns like `meeting_key` and `driver_name` from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature overview\n",
    "feature_cols = [col for col in df.columns if col not in ['meeting_key', 'driver_name', 'finishing_position', 'points']]\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(\"\\nFeatures:\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualization\n",
    "\n",
    "Visualizing the data helps us understand the underlying patterns and relationships before modeling. It also helps identify any data quality issues or biases.\n",
    "\n",
    "### 3.1 Distribution of Positions\n",
    "\n",
    "We examine the distribution of finishing positions. In a perfectly balanced dataset without DNFs (Did Not Finish), this would be uniform (equal counts for positions 1-20). The Grid Position distribution shows where drivers started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df['finishing_position'].hist(bins=20, edgecolor='black')\n",
    "plt.xlabel('Finishing Position')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Finishing Positions')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['grid_position'].hist(bins=20, edgecolor='black', color='orange')\n",
    "plt.xlabel('Grid Position')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Starting Positions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Grid Position vs Finishing Position Correlation\n",
    "\n",
    "Historically, starting position is the strongest predictor of race results. The scatter plot below visualizes this relationship. Points along the red dashed diagonal line represent races where the driver finished exactly where they started. Points above the line indicate lost positions, while points below indicate gained positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['grid_position'], df['finishing_position'], alpha=0.5)\n",
    "plt.plot([1, 20], [1, 20], 'r--', label='Perfect correlation')\n",
    "plt.xlabel('Grid Position (Starting)')\n",
    "plt.ylabel('Finishing Position')\n",
    "plt.title('Grid Position vs Finishing Position')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "correlation = df[['grid_position', 'finishing_position']].corr().iloc[0, 1]\n",
    "print(f\"Correlation: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Correlation Heatmap\n",
    "\n",
    "This heatmap displays the correlation coefficients between key features. \n",
    "- **Red/Warm colors** indicate positive correlation (as one increases, the other increases).\n",
    "- **Blue/Cool colors** indicate negative correlation.\n",
    "\n",
    "We look for features strongly correlated with `finishing_position`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select key features for visualization\n",
    "key_features = ['practice_best_lap', 'quali_best_lap', 'grid_position', \n",
    "                'practice_avg_i1_speed', 'practice_avg_i2_speed', 'finishing_position']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df[key_features].corr(), annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Key Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Practice vs Qualifying Lap Times\n",
    "\n",
    "We compare the best lap times from Practice sessions against Qualifying. A strong linear relationship suggests that practice performance is a reliable indicator of qualifying potential, which ultimately determines grid position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['practice_best_lap'], df['quali_best_lap'], alpha=0.5)\n",
    "plt.xlabel('Practice Best Lap (seconds)')\n",
    "plt.ylabel('Qualifying Best Lap (seconds)')\n",
    "plt.title('Practice vs Qualifying Performance')\n",
    "plt.plot([df['practice_best_lap'].min(), df['practice_best_lap'].max()],\n",
    "         [df['practice_best_lap'].min(), df['practice_best_lap'].max()],\n",
    "         'r--', alpha=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network Architecture\n",
    "\n",
    "We employ a **Deep Feedforward Neural Network** (Multilayer Perceptron) designed for regression.\n",
    "\n",
    "### Architecture Details:\n",
    "- **Input Layer:** 22 neurons (corresponding to the 22 input features).\n",
    "- **Hidden Layers:** \n",
    "  - Layer 1: 128 neurons + Batch Normalization + ReLU + Dropout (30%)\n",
    "  - Layer 2: 64 neurons + Batch Normalization + ReLU + Dropout (30%)\n",
    "  - Layer 3: 32 neurons + Batch Normalization + ReLU + Dropout (20%)\n",
    "- **Output Layer:** 1 neuron (predicts continuous position score).\n",
    "\n",
    "**Why this structure?** The tapering size (128 -> 64 -> 32) forces the network to learn increasingly abstract representations. Batch Normalization stabilizes training, and Dropout prevents overfitting by randomly disabling neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class F1DeepPredictor(nn.Module):\n",
    "    def __init__(self, input_features=22):\n",
    "        super(F1DeepPredictor, self).__init__()\n",
    "        \n",
    "        # Layer 1: Input -> 128\n",
    "        self.fc1 = nn.Linear(input_features, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Layer 2: 128 -> 64\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Layer 3: 64 -> 32\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        # Output: 32 -> 1 (regression)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Create model and show architecture\n",
    "model = F1DeepPredictor(input_features=22)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Trained Model and Visualize Performance\n",
    "\n",
    "We load the saved model state (`f1_model.pth`), which includes the trained weights and the `StandardScaler` used during training. It's critical to use the exact same scaler for new data to ensure features are normalized correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load trained model\n",
    "checkpoint = torch.load('f1_model.pth', weights_only=False)\n",
    "model = F1DeepPredictor(input_features=22)\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "model.eval()\n",
    "\n",
    "scaler = checkpoint['scaler']\n",
    "feature_cols = checkpoint['features']\n",
    "\n",
    "print(\"✓ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Make Predictions on Test Data\n",
    "\n",
    "We run the model on the test dataset. The model outputs a continuous float value (e.g., 3.4), which we round to the nearest integer to determine the predicted finishing position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "X = df[feature_cols].values\n",
    "y = df['finishing_position'].values.astype(float)\n",
    "\n",
    "# Handle NaN\n",
    "nan_mask = np.isnan(X)\n",
    "if nan_mask.any():\n",
    "    col_means = np.nanmean(X, axis=0)\n",
    "    for i in range(X.shape[1]):\n",
    "        X[nan_mask[:, i], i] = col_means[i]\n",
    "\n",
    "# Scale\n",
    "X_scaled = scaler.transform(X)\n",
    "X_scaled = np.clip(X_scaled, -5, 5)\n",
    "\n",
    "# Split\n",
    "_, X_test, _, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    predictions_raw = model(X_test_tensor).squeeze().numpy()\n",
    "    predictions = np.clip(np.round(predictions_raw), 1, 20)\n",
    "\n",
    "print(f\"Test set size: {len(X_test)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prediction vs Actual Scatter Plot\n",
    "\n",
    "This plot compares the model's predictions (Y-axis) against the actual race results (X-axis). \n",
    "- **Ideal Scenario:** All points lie on the red dashed diagonal line.\n",
    "- **Interpretation:** The tightness of the cluster around the line indicates the model's precision. Outliers represent races where the result was significantly different from what performance metrics suggested (e.g., due to accidents or mechanical failures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, predictions, alpha=0.5)\n",
    "plt.plot([1, 20], [1, 20], 'r--', label='Perfect prediction')\n",
    "plt.xlabel('Actual Position')\n",
    "plt.ylabel('Predicted Position')\n",
    "plt.title('Model Predictions vs Actual Results')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 21)\n",
    "plt.ylim(0, 21)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Error Distribution\n",
    "\n",
    "We analyze the errors (Predicted - Actual).\n",
    "- **Left Plot (Error):** Shows the direction of errors. A peak at 0 is desired.\n",
    "- **Right Plot (Absolute Error):** Shows the magnitude of errors. We want the majority of the mass to be near 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "errors = predictions - y_test\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(errors, bins=30, edgecolor='black')\n",
    "plt.xlabel('Prediction Error (positions)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.axvline(x=0, color='r', linestyle='--', label='Perfect prediction')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "abs_errors = np.abs(errors)\n",
    "plt.hist(abs_errors, bins=20, edgecolor='black', color='orange')\n",
    "plt.xlabel('Absolute Error (positions)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Absolute Errors')\n",
    "plt.axvline(x=abs_errors.mean(), color='r', linestyle='--', label=f'Mean: {abs_errors.mean():.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Performance Metrics\n",
    "\n",
    "We quantify performance using standard regression metrics:\n",
    "- **MAE (Mean Absolute Error):** The average number of positions the model is off by.\n",
    "- **Within ±N:** The percentage of predictions that are within N positions of the actual result. This is often the most practical metric for race strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate metrics\n",
    "exact_accuracy = (predictions == y_test).sum() / len(y_test) * 100\n",
    "mae = np.abs(errors).mean()\n",
    "within_1 = (np.abs(errors) <= 1).sum() / len(errors) * 100\n",
    "within_2 = (np.abs(errors) <= 2).sum() / len(errors) * 100\n",
    "within_3 = (np.abs(errors) <= 3).sum() / len(errors) * 100\n",
    "\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Exact Accuracy:          {exact_accuracy:.2f}%\")\n",
    "print(f\"Mean Absolute Error:     {mae:.2f} positions\")\n",
    "print(f\"Within ±1 position:      {within_1:.2f}%\")\n",
    "print(f\"Within ±2 positions:     {within_2:.2f}%\")\n",
    "print(f\"Within ±3 positions:     {within_3:.2f}%\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Accuracy by Position Range\n",
    "\n",
    "Races are dynamic. It is often easier to predict the podium finishers (Top 5) and the backmarkers than the highly competitive midfield. This chart breaks down the model's accuracy (within ±2 positions) across different segments of the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Group by position ranges\n",
    "position_ranges = [(1, 5, 'Top 5'), (6, 10, 'Mid 6-10'), (11, 15, 'Mid 11-15'), (16, 20, 'Bottom 16-20')]\n",
    "\n",
    "accuracies = []\n",
    "labels = []\n",
    "\n",
    "for start, end, label in position_ranges:\n",
    "    mask = (y_test >= start) & (y_test <= end)\n",
    "    if mask.sum() > 0:\n",
    "        within_2_pct = (np.abs(errors[mask]) <= 2).sum() / mask.sum() * 100\n",
    "        accuracies.append(within_2_pct)\n",
    "        labels.append(label)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, accuracies, color=['gold', 'silver', 'bronze', 'gray'], edgecolor='black')\n",
    "plt.xlabel('Position Range')\n",
    "plt.ylabel('Accuracy within ±2 positions (%)')\n",
    "plt.title('Model Accuracy by Position Range')\n",
    "plt.ylim(0, 100)\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis\n",
    "\n",
    "Understanding *why* the model makes a prediction is crucial. We analyze feature importance by calculating the correlation between each input feature and the target variable. Features with higher absolute correlation values are more influential in the model's decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate correlation with finishing position\n",
    "correlations = []\n",
    "for col in feature_cols:\n",
    "    corr = df[[col, 'finishing_position']].corr().iloc[0, 1]\n",
    "    correlations.append((col, abs(corr)))\n",
    "\n",
    "correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Plot top 10 features\n",
    "top_features = correlations[:10]\n",
    "feature_names = [f[0] for f in top_features]\n",
    "feature_corrs = [f[1] for f in top_features]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_names, feature_corrs, color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Absolute Correlation with Finishing Position')\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Features:\")\n",
    "for i, (name, corr) in enumerate(top_features, 1):\n",
    "    print(f\"{i:2d}. {name:<35} {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real Race Prediction Example\n",
    "\n",
    "The ultimate test is predicting a new race. The script `test_sao_paulo_gp.py` implements the full pipeline: fetching live data from the OpenF1 API, processing it into features, and generating predictions. Below is an example of the output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# This would fetch from OpenF1 API - simplified here\n",
    "print(\"To make predictions for a new race:\")\n",
    "print(\"\\n1. Use test_sao_paulo_gp.py script\")\n",
    "print(\"2. Or provide CSV with 22 required features\")\n",
    "print(\"3. Model will rank drivers and assign unique positions 1-20\")\n",
    "print(\"\\nExample output:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Driver                   Grid    Predicted\")\n",
    "print(\"-\" * 50)\n",
    "print(\"#12 ANT (Antonelli)     P2      P1\")\n",
    "print(\"#81 PIA (Piastri)       P5      P2\")\n",
    "print(\"#87 BEA (Bearman)       P3      P3\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Grid position is the strongest predictor** - Correlation ~0.7 with finishing position\n",
    "2. **Qualifying performance matters more than practice** - quali_best_lap has higher correlation\n",
    "3. **Model performs best for mid-field positions** - Top and bottom positions have more variability\n",
    "4. **75.85% accuracy within ±3 positions** - Good for predicting general race outcome\n",
    "\n",
    "### Model Architecture:\n",
    "- **Type:** Deep Feedforward Neural Network (Regression)\n",
    "- **Layers:** 4 (3 hidden + 1 output)\n",
    "- **Parameters:** ~20,000 trainable parameters\n",
    "- **Regularization:** Batch Normalization + Dropout\n",
    "\n",
    "### Future Improvements:\n",
    "1. Add weather data (rain significantly affects results)\n",
    "2. Include driver/team historical performance\n",
    "3. Add track-specific features\n",
    "4. Ensemble multiple models\n",
    "5. More training data (2018-2022 seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Project Files\n",
    "\n",
    "- `f1_model.pth` - Trained model\n",
    "- `f1_deep_neural_network.py` - Model architecture\n",
    "- `02_feature_engineering.py` - Feature engineering pipeline\n",
    "- `03_train_model.py` - Training script\n",
    "- `test_sao_paulo_gp.py` - Prediction script\n",
    "- `training_dataset.csv` - Processed training data\n",
    "\n",
    "**End of Documentation**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
